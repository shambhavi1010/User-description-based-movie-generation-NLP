# -*- coding: utf-8 -*-
"""NLP Movie Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fYVrXuYuMKfnoKMAAtSNfRiZTsfM27XI
"""

!pip install falcon tensorflow gunicorn

!pip install yake

!pip install contractions

!pip install keras-tuner

"""## IMPORTING LIBRARIES"""

import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import nltk
import re
import contractions

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import re
import contractions
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from tensorflow.keras.layers import Dropout, LSTM, Dense, Embedding
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras import Sequential
from sklearn.model_selection import train_test_split
import kerastuner as kt

import nltk
nltk.download("all")

"""## IMPORTING DATASET"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_excel('/content/drive/MyDrive/NLP SEM 5 project/marathi_movies_combined_final.xlsx')

print(data)

# Create a 'corpus' column with all relevant fields except the movie title
data['corpus'] = data.apply(lambda row: f"{row['director']} {row['cast']} {row['genre']} {row['production_house']} {row['notes']} {row['imdb_description']}", axis=1)

# Create a 'target' column with just the movie title
data['target'] = data['title']

"""##**TEXT** **PREPROCESSING**

## TFIDF
"""

# Define text columns for preprocessing
text_columns = ['title', 'director', 'cast', 'genre', 'production_house', 'notes', 'imdb_description']

# Function to preprocess text for the corpus
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Expand contractions
    text = contractions.fix(text)
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    words = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words)

# Apply preprocessing to each text column in the corpus
for col in text_columns:
    if col in data.columns:
        data[col] = data[col].astype(str).apply(preprocess_text)

# Create a 'corpus' column with all relevant fields except the movie title
data['corpus'] = data.apply(lambda row: ' '.join([str(row[col]) for col in text_columns if col != 'title']), axis=1)

# Create a 'target' column with just the movie title
data['target'] = data['title']

print("Preprocessing complete. Data is ready for the chatbot.")

# Apply TF-IDF to 'corpus'
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(data['corpus'])

print("TF-IDF applied. Data is ready for the next step.")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(data['corpus'])

# Calculate similarity matrix
cosine_sim_text = linear_kernel(tfidf_matrix, tfidf_matrix)



"""## WORD2VEC"""

import gensim

import numpy as np
from gensim.models import KeyedVectors

# Load pre-trained Word2Vec model (for example, Google's pre-trained model)
word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)

# Function to get the average word vector for a document
def get_average_word2vec(doc):
    words = doc.split()
    word_vectors = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]
    if not word_vectors:
        return np.zeros(300)
    return np.mean(word_vectors, axis=0)

# Apply Word2Vec Embeddings
data['word2vec_embedding'] = data['corpus'].apply(get_average_word2vec).tolist()

print("Word2Vec Embeddings are ready.")

"""##BERT EMBEDDING"""

pip install transformers

import torch
from transformers import AutoModel, AutoTokenizer

# Load pre-trained BERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

# Function to get BERT embeddings
def get_bert_embedding(doc):
    inputs = tokenizer(doc, return_tensors='pt', truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embeddings

# Apply BERT Embeddings
data['bert_embedding'] = data['corpus'].apply(get_bert_embedding).tolist()

print("BERT Embeddings are ready.")



"""## **TRAINING MODELS**

##TFIDF WITH LSTM
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Tokenize the text
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(data['corpus'])

# Convert text to sequences and pad the sequences
sequences = tokenizer.texts_to_sequences(data['corpus'])
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')

# Convert target to sequences
target_sequences = tokenizer.texts_to_sequences(data['target'])

# Since we're using sparse categorical crossentropy,
# we only need the index of the target word, not the full sequence
target_index = [seq[0] if seq else 0 for seq in target_sequences]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, target_index, test_size=0.2, random_state=42)

# Define the RNN model
model = Sequential([
    Embedding(input_dim=10000, output_dim=64, input_length=100),
    LSTM(64, return_sequences=True),
    LSTM(64),
    Dense(10000, activation='softmax')  # Output layer size should match the number of words in the tokenizer
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, np.array(y_train), validation_data=(X_test, np.array(y_test)), epochs=10,)

print("Model training complete.")

model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)

# Save the model weights to an H5 file
model.save_weights("model.h5")
print("Model saved to disk.")

from sklearn.metrics import accuracy_score

predictions = model.predict(X_test)

# Convert predictions to label indices
predicted_indices = np.argmax(predictions, axis=1)

# Convert predicted indices to words
predicted_words = [tokenizer.index_word.get(index, "UNKNOWN") for index in predicted_indices]

# Convert true labels to words
true_words = [tokenizer.index_word.get(index, "UNKNOWN") for index in y_test]

# Calculate accuracy
accuracy = accuracy_score(true_words, predicted_words)
print("Test Accuracy:", accuracy)

model.save("bestmodel1.h5")

import json
from keras.preprocessing.text import tokenizer_from_json

#  'tokenizer' is your Tokenizer object
tokenizer_json = tokenizer.to_json()

with open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test, np.array(y_test))
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

from sklearn.metrics import precision_score, recall_score, f1_score

predictions = model.predict(X_test, verbose=1)
# Convert predictions to label indices
predicted_indices = np.argmax(predictions, axis=1)

# Calculate precision, recall, and F1 score
precision = precision_score(y_test, predicted_indices, average='weighted')
recall = recall_score(y_test, predicted_indices, average='weighted')
f1 = f1_score(y_test, predicted_indices, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""## LSTM"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import accuracy_score

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
tfidf_matrix = vectorizer.fit_transform(data['corpus']).toarray()

# Encode the labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['title'])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)
# Build the model
model = Sequential([
    Dense(256, input_shape=(X_train.shape[1],), activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model with a smaller batch size and more epochs
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

# Make predictions on the test set
predictions = model.predict(X_test)

# Convert predictions to label indices
predicted_indices = np.argmax(predictions, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test, predicted_indices)
print("Test Accuracy:", test_accuracy)

"""## WORD2VEC RNN

"""

from gensim.models import KeyedVectors
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score

# Tokenize and pad the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['corpus'])
sequences = tokenizer.texts_to_sequences(data['corpus'])
padded_sequences = pad_sequences(sequences, maxlen=200)

# Encode the labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['target'])

# Create an embedding matrix
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 300
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    if word in word2vec_model.key_to_index:
        embedding_vector = word2vec_model[word]
        embedding_matrix[i] = embedding_vector

# Split the data
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Build the RNN model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=200, trainable=False),
    LSTM(64, return_sequences=True),
    LSTM(32),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),shuffle=True)

"""## WORD2VEC lstm

"""

from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.initializers import Constant
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import accuracy_score
import gensim

# Tokenize the corpus
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['corpus'])
sequences = tokenizer.texts_to_sequences(data['corpus'])
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

# Padding sequences
data_pad = pad_sequences(sequences, maxlen=200)
print('Shape of data tensor:', data_pad.shape)

# Encode labels
labels = data['target']
le = LabelEncoder()
labels = le.fit_transform(labels)
print('Shape of label tensor:', labels.shape)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data_pad, labels, test_size=0.2, random_state=42)

# Prepare the embedding matrix
num_words = len(word_index) + 1
embedding_matrix = np.zeros((num_words, 300))
for word, i in word_index.items():
    if i > num_words:
        continue
    embedding_vector = word2vec_model[word] if word in word2vec_model.key_to_index else None
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# Build the LSTM Model
model = Sequential()
embedding_layer = Embedding(num_words,
                            300,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=200,
                            trainable=False)
model.add(embedding_layer)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(len(le.classes_), activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), verbose=2)

# Evaluate the model
predictions = model.predict(X_test)
predicted_indices = np.argmax(predictions, axis=1)
test_accuracy = accuracy_score(y_test, predicted_indices)
print("Test Accuracy:", test_accuracy)

"""BERT"""

import torch
from torch import nn
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np

# 1. Prepare Data for Training
labels = data['target'].astype(str).values
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

bert_embeddings = np.array(data['bert_embedding'].tolist())

X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, labels_encoded, test_size=0.2, random_state=42)

train_data = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
test_data = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))

batch_size = 32
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# 2. Build Classification Model
class BERTClassifier(nn.Module):
    def __init__(self, n_classes):
        super(BERTClassifier, self).__init__()
        self.linear1 = nn.Linear(768, 512)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.linear2 = nn.Linear(512, n_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return self.softmax(x)

n_classes = len(np.unique(labels_encoded))
model = BERTClassifier(n_classes=n_classes)

# 3. Training
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # Adjusts the learning rate
loss_fn = nn.CrossEntropyLoss()

epochs = 100
for epoch in range(epochs):
    model.train()
    total_loss, total_accuracy = 0, 0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
    scheduler.step()  # Adjusts the learning rate
    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')

# 4. Evaluation
model.eval()
total_accuracy = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        predictions = torch.argmax(outputs, dim=1)
        total_accuracy += (predictions == labels).sum().item()
accuracy = total_accuracy / len(y_test)
print(f'Test Accuracy: {accuracy:.4f}')

"""## TFIDF BERT"""

import torch
from torch import nn
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Prepare Data for Training
corpus = data['corpus'].astype(str).values  # Assuming 'text' column contains the text data
labels = data['target'].astype(str).values

label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

# Check class distribution
sns.countplot(labels_encoded)
plt.title('Class Distribution')
plt.show()

# Generate TF-IDF embeddings
vectorizer = TfidfVectorizer(max_features=768)  # Limiting to 768 features for simplicity
tfidf_embeddings = vectorizer.fit_transform(corpus).toarray()

# Check Embeddings
print('Embeddings:', tfidf_embeddings.shape)
print('Non-zero elements in embeddings:', np.count_nonzero(tfidf_embeddings))

X_train, X_test, y_train, y_test = train_test_split(tfidf_embeddings, labels_encoded, test_size=0.2, random_state=42)

train_data = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
test_data = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))

batch_size = 32
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# 2. Build Classification Model
class TFIDFClassifier(nn.Module):
    def __init__(self, n_classes, input_size):
        super(TFIDFClassifier, self).__init__()
        self.linear1 = nn.Linear(input_size, 512)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.linear2 = nn.Linear(512, n_classes)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

n_classes = len(np.unique(labels_encoded))
model = TFIDFClassifier(n_classes=n_classes, input_size=768)

# 3. Training
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Increased learning rate
scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # Adjusts the learning rate
loss_fn = nn.CrossEntropyLoss()

epochs = 250
for epoch in range(epochs):
    model.train()
    total_loss, total_accuracy = 0, 0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

        predictions = torch.argmax(outputs, dim=1)
        total_accuracy += (predictions == labels).float().mean()

    scheduler.step()  # Adjusts the learning rate
    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Training Accuracy: {total_accuracy / len(train_loader)}')

# 4. Evaluation
model.eval()
total_accuracy = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        predictions = torch.argmax(outputs, dim=1)
        total_accuracy += (predictions == labels).float().mean()
accuracy = total_accuracy / len(test_loader)
print(f'Test Accuracy: {accuracy:.4f}')

"""##MAML"""

import torch
from torch import nn, optim
from learn2learn.algorithms import MAML
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import TensorDataset, DataLoader

import learn2learn as l2l

fast_lr = 0.01  # You should choose an appropriate value for your use case
maml = l2l.algorithms.MAML(model, lr=fast_lr)


# Assuming your DataFrame 'data' has 'corpus' and 'target' columns
labels = data['target'].astype(str).values
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)
bert_embeddings = np.array(data['bert_embedding'].tolist())

# Define Tasks (This is a simplified example; you might need to define multiple tasks based on your needs)
task_1_data, task_2_data, task_1_labels, task_2_labels = train_test_split(bert_embeddings, labels_encoded, test_size=0.5, random_state=42)

# Create DataLoaders for Each Task
batch_size = 32
task_1_dataset = TensorDataset(torch.FloatTensor(task_1_data), torch.LongTensor(task_1_labels))
task_1_loader = DataLoader(task_1_dataset, batch_size=batch_size, shuffle=True)

task_2_dataset = TensorDataset(torch.FloatTensor(task_2_data), torch.LongTensor(task_2_labels))
task_2_loader = DataLoader(task_2_dataset, batch_size=batch_size, shuffle=True)

# Define Model
class Classifier(nn.Module):
    def __init__(self, input_size, n_classes):
        super(Classifier, self).__init__()
        self.linear = nn.Linear(input_size, n_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.linear(x)
        return self.softmax(x)

n_classes = len(np.unique(labels_encoded))
model = Classifier(768, n_classes)  # 768 is the size of BERT embeddings

# Define MAML
maml = MAML(model, lr=1e-3)

# Define Loss and Optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(maml.parameters(), lr=1e-3)

# MAML Training Loop
epochs = 10
for epoch in range(epochs):
    optimizer.zero_grad()
    task_1_loss, task_2_loss = 0.0, 0.0
    for task_1_batch, task_2_batch in zip(task_1_loader, task_2_loader):
        task_1_data, task_1_labels = task_1_batch
        task_2_data, task_2_labels = task_2_batch

        # Fast adapt to task 1
        learner = maml.clone()
        task_1_pred = learner(task_1_data)
        task_1_loss = loss_fn(task_1_pred, task_1_labels)
        learner.adapt(task_1_loss)

        # Compute task 2 loss
        task_2_pred = learner(task_2_data)
        task_2_loss = loss_fn(task_2_pred, task_2_labels)

    # Meta-update the model parameters
    total_loss = task_1_loss + task_2_loss
    total_loss.backward()
    optimizer.step()



"""##RNN WITH VOCABULARY AS TRAINNING


"""

from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, TimeDistributed

# Prepare 'corpus' data
tokenizer_corpus = Tokenizer()
tokenizer_corpus.fit_on_texts(data['corpus'])
vocab_size = len(tokenizer_corpus.word_index) + 1
sequences_corpus = tokenizer_corpus.texts_to_sequences(data['corpus'])

# Prepare 'title' data
tokenizer_title = Tokenizer()
tokenizer_title.fit_on_texts(data['title'])
sequences_title = tokenizer_title.texts_to_sequences(data['title'])
title_vocab_size = len(tokenizer_title.word_index) + 1

# Find max sequence length
max_seq_length = max(max([len(seq) for seq in sequences_corpus]), max([len(seq) for seq in sequences_title]))

# Pad sequences
X = pad_sequences(sequences_corpus, maxlen=max_seq_length, padding='post')
y = pad_sequences(sequences_title, maxlen=max_seq_length, padding='post')

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Expand dimensions of 'y' to match Keras expectations
y_train = np.expand_dims(y_train, -1)
y_val = np.expand_dims(y_val, -1)

# Define model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length))
model.add(LSTM(128, return_sequences=True))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(title_vocab_size, activation='softmax')))

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))

from sklearn.metrics import accuracy_score

# Assuming y_val_pred is your model predictions on the validation set
y_val_pred = model.predict(X_val)

# Convert predictions to label indices
y_val_pred_indices = np.argmax(y_val_pred, axis=-1)

# Flatten predictions and true values for accuracy computation
y_val_flat = y_val.squeeze(-1).flatten()
y_val_pred_flat = y_val_pred_indices.flatten()

# Compute accuracy
accuracy = accuracy_score(y_val_flat, y_val_pred_flat)
print('Test Accuracy:', accuracy)

model.save("vocabmodel.json")

model.save("vocabmodel.h5")

def generate_response(seed_text, max_seq_length, model, tokenizer_corpus, tokenizer_title):
    for _ in range(20):  # limiting response length to 20 for simplicity
        token_list = tokenizer_corpus.texts_to_sequences([seed_text])[0]
        print("Token List:", token_list)  # Debugging line
        token_list = pad_sequences([token_list], maxlen=max_seq_length, padding='post')
        predicted = model.predict(token_list, verbose=0)
        predicted_index = np.argmax(predicted, axis=-1)[0, -1]
        output_word = ''
        for word, index in tokenizer_title.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        if output_word == 'endseq':  # assuming 'endseq' is a token that signifies the end of a sequence
            break
        seed_text += " " + output_word
    return seed_text

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    response = generate_response(user_input, max_seq_length, model, tokenizer_corpus, tokenizer_title)
    print("Bot:", response)

"""## FINE TUNING"""

from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, TimeDistributed
from tensorflow.keras.regularizers import L2

from sklearn.model_selection import KFold

# Prepare 'corpus' data
tokenizer_corpus = Tokenizer()
tokenizer_corpus.fit_on_texts(data['corpus'])
vocab_size = len(tokenizer_corpus.word_index) + 1
sequences_corpus = tokenizer_corpus.texts_to_sequences(data['corpus'])

# Prepare 'title' data
tokenizer_title = Tokenizer()
tokenizer_title.fit_on_texts(data['title'])
sequences_title = tokenizer_title.texts_to_sequences(data['title'])
title_vocab_size = len(tokenizer_title.word_index) + 1

# Find max sequence length
max_seq_length = max(max([len(seq) for seq in sequences_corpus]), max([len(seq) for seq in sequences_title]))

# Pad sequences
X = pad_sequences(sequences_corpus, maxlen=max_seq_length, padding='post')
y = pad_sequences(sequences_title, maxlen=max_seq_length, padding='post')

# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Expand dimensions of 'y' to match Keras expectations
y_train = np.expand_dims(y_train, -1)
y_val = np.expand_dims(y_val, -1)

n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

fold_no = 1
for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Define model
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length))
    model.add(LSTM(128, return_sequences=True))
    model.add(LSTM(128, return_sequences=True, kernel_regularizer=L2(0.01)))
    model.add(Dropout(0.5))
    model.add(TimeDistributed(Dense(title_vocab_size, activation='softmax')))

    # Compile model
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()

    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')
    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))
    fold_no += 1